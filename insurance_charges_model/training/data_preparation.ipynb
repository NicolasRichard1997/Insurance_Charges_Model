{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicolas/.local/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "import numpy as np\n",
    "from numpy import inf, nan\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import featuretools as ft\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "current_directory = os.getcwd()\n",
    "repertory_path = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from insurance_charges_model.prediction.transformers import DFSTransformer\n",
    "from insurance_charges_model.prediction.transformers import InfinityToNaNTransformer\n",
    "from insurance_charges_model.prediction.transformers import IntToFloatTransformer\n",
    "from insurance_charges_model.prediction.transformers import BooleanTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(repertory_path + \"/data/insurance.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Feature Synthesis\n",
    "We'll be using the featuretools package to do feature engineering.\n",
    "\n",
    "An EntitySet is an object that we will give to the featuretools package in order to do feature engineering. An entitySet denotes the features of specific \"entity\" in the real world. In this case, we will work with only one type of entity: \"transactions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: Transactions\n",
       "  DataFrames:\n",
       "    Transactions [Rows: 1338, Columns: 8]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityset = ft.EntitySet(id=\"Transactions\")\n",
    "entityset = entityset.add_dataframe(dataframe_name=\"Transactions\",\n",
    "                                    dataframe=df,\n",
    "                                    make_index=True,\n",
    "                                    index=\"index\")\n",
    "entityset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'age', 'sex', 'bmi', 'children', 'smoker', 'region',\n",
       "       'charges'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a list of variables associated with the EntitySet we just created\n",
    "entityset[\"Transactions\"].columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined an EntitySet for our data, we'll ask the featuretools package to create some features for us. The package defines a set of \"primitives\" that are able to create new features by processing the features that already exist in the EntitySet.\n",
    "\n",
    "We are also going to ignore the categorical and boolean features in the dataset because they don't play well with the numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature_dataframe variable now contains the new features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Features using Deep Feature Synthesis\n",
    "\n",
    "Now we can create a Transformer that we can use later to create the features, given samples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insurance_charges_model.prediction.transformers import DFSTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_entity = \"Transactions\"\n",
    "trans_primitives = [\"add_numeric\", \"subtract_numeric\", \"multiply_numeric\", \"divide_numeric\", \"greater_than\", \"less_than\"]\n",
    "ignore_variables = {\"Transactions\": [\"sex\", \"smoker\", \"region\"]}\n",
    "\n",
    "dfs_transformer = DFSTransformer(target_entity=target_entity, trans_primitives=trans_primitives, ignore_variables=ignore_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transformer for inf Values\n",
    "\n",
    "Some of the features created by the featuretools package have a value of 'inf'. We'll create a transformer that maps these values to 0.0 to allow the models to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "infinity_transformer = InfinityToNaNTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [nan]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "# testing the transformer\n",
    "inpt = [[1.0], [inf], [1.0]]\n",
    "\n",
    "# copying the transformer object in order to fit and test it\n",
    "infinity_transformer_copy = deepcopy(infinity_transformer)\n",
    "\n",
    "infinity_transformer_copy.fit(inpt)\n",
    "result = infinity_transformer_copy.transform(inpt)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle the NaN values, we'll use a SimpleImputer that will fill in the missing value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# testing the transformer\n",
    "\n",
    "# copying the transformer object in order to fit and test it\n",
    "simple_imputer_copy = deepcopy(simple_imputer)\n",
    "\n",
    "simple_imputer_copy.fit(result)\n",
    "\n",
    "test_df = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\n",
    "print(simple_imputer_copy.transform(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleImputer transformer has problems with imputing values that are not floats when using the 'mean' strategy. To fix this, we'll create a transformer that will convert all integer columns into floating point columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_float_transformer = IntToFloatTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  3.]\n",
      " [ 4.  6.]\n",
      " [10.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# test the transformer\n",
    "\n",
    "# copying the transformer object in order to fit and test it\n",
    "int_to_float_transformer_copy = deepcopy(int_to_float_transformer)\n",
    "\n",
    "int_to_float_transformer_copy.fit(result)\n",
    "\n",
    "test_df = [[2, 3.0], [4, 6.0], [10, 9.0]]\n",
    "print(int_to_float_transformer_copy.transform(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll put the IntToFloatTransformer, DFSTransformer, InfinityToNaNTransformer, and SimpleImputer transformers into a Pipeline so they'll all work together as a unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pipeline = Pipeline([\n",
    "    (\"dfs_transformer\", dfs_transformer),\n",
    "    (\"int_to_float_transformer\", int_to_float_transformer),\n",
    "    (\"infinity_transformer\", infinity_transformer),\n",
    "    (\"simple_imputer\", simple_imputer),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityset = ft.EntitySet(id=\"Transactions\")\n",
    "entityset = entityset.add_dataframe(dataframe_name=\"Transactions\",\n",
    "                                    dataframe=df,\n",
    "                                    make_index=True,\n",
    "                                    index=\"index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Boolean Features\n",
    "\n",
    "We'll create a transformer that is able to convert the string in the 'smoker' feature to a boolean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_transformer = BooleanTransformer(true_value=\"yes\", false_value=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the transformer\n",
    "test_df = pd.DataFrame([[\"yes\"], [\"no\"], [\"yes\"]],\n",
    "                       columns=[\"smoker\"])\n",
    "\n",
    "# copying the transformer object in order to fit and test it\n",
    "boolean_transformer_copy = deepcopy(boolean_transformer)\n",
    "\n",
    "boolean_transformer_copy.fit(test_df)\n",
    "result = boolean_transformer_copy.transform(test_df)\n",
    "\n",
    "if (result != np.array([[True], [False], [True]])).all():\n",
    "    raise ValueError(\"Unexpected values found in array.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Features\n",
    "\n",
    "Next, we'll create an encoder that will encode the categorical features. The categorical features that we will encode will be 'sex' and 'region'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the transformer\n",
    "test_df = pd.DataFrame([[\"southwest\"], [\"northeast\"], [\"southwest\"]],\n",
    "                       columns=[\"region\"])\n",
    "\n",
    "# copying the transformer object in order to fit and test it\n",
    "ordinal_encoder_copy = deepcopy(ordinal_encoder)\n",
    "\n",
    "ordinal_encoder_copy.fit(test_df)\n",
    "result = ordinal_encoder_copy.transform(test_df)\n",
    "\n",
    "if (result != np.array([[1.0], [0.0], [1.0]])).all():\n",
    "    raise ValueError(\"Unexpected values found in array.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ColumnTransformer\n",
    "\n",
    "Combining all of the preprocessors into one ColumnTransformer that can be used to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(\n",
    "    remainder=\"passthrough\",\n",
    "    transformers=[\n",
    "        (\"dfs_pipeline\", dfs_pipeline, [\"age\", \"sex\", \"bmi\", \"children\", \"smoker\", \"region\"]),\n",
    "        (\"boolean_transformer\", boolean_transformer, [\"smoker\"]),\n",
    "        (\"ordinal_encoder\", ordinal_encoder, [\"sex\", \"region\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DFSTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_entity=None):\n",
    "        self.target_entity = target_entity\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # your transformation code here\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving ColumnTransformer\n",
    "\n",
    "NOTE: the ColumnTransformer object is saved in an UNFITTED state, it will be fitted to the data set later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(column_transformer, \"transformer.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
